# -*- coding: utf-8 -*-
"""topic bert compare Coherence TopicModeling

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MevUeDFz67PsXWVf0mIeNartxwXj2oew
"""

from google.colab import files
uploaded = files.upload()

"""# **The Data**"""

with open('1.txt') as f:
  documents = [line.rstrip('\n') for line in f]

#Take a peek of the data.

print(len(documents))
print(documents[:6])

"""# **Data Pre-processing**"""

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *

import numpy as np
np.random.seed(2018)

import nltk
nltk.download('wordnet')
stemmer=SnowballStemmer

# function to perform lemmatize and stem preprocessing

stemmer = SnowballStemmer("english")

def lemmatize_stemming(text):
  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
  result = []
  for token in gensim.utils.simple_preprocess(text):
    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:   ###***
      result.append(lemmatize_stemming(token))
  return result

"""# **Bag of Words on the Data set**

Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set.
"""

dictionary = gensim.corpora.Dictionary(documents)

count = 0
for k, v in dictionary.iteritems():
    print(k, v)
    count += 1
    if count > 15:
        break

"""***Gensim filter_extremes***

**Filter out tokens that appear in**

less than 15 documents (absolute number) or
more than 0.5 documents (fraction of total corpus size, not absolute number).
after the above two steps, keep only the first 100000 most frequent tokens.
"""

dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)

#Gensim doc2bow

bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
bow_corpus[:15]

#Preview Bag Of Words for our sample preprocessed document

bow_doc_4310 = bow_corpus[8]
for i in range(len(bow_doc_4310)):
  print("Word {} (\"{}\") appears {} time.".format(bow_doc_4310[i][0],dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))

"""# **TF-IDF**

Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document.
"""

from gensim import corpora, models   #****

tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]

from pprint import pprint

for doc in corpus_tfidf:
  pprint(doc)
  break

"""# **Running LDA using Bag of Words**

*   List item
*   List item



Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’
"""

lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)

#For each topic, we will explore the words occuring in that topic and its relative weight

for idx, topic in lda_model.print_topics(-1):
  print('Topic: {} \nWords: {}'.format(idx, topic))

"""# **Running LDA using TF-IDF**"""

lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)

for idx, topic in lda_model_tfidf.print_topics(-1):
  print('Topic: {} Word: {}'.format(idx, topic))

"""# **Performance evaluation by classifying sample document using LDA Bag of Words model**"""

processed_docs[4310]

for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):
  print("\nScore: {}\t \nTopic: {}".format(score, lda_model.print_topic(index, 10)))

"""# **Performance evaluation by classifying sample document using LDA TF-IDF model.**"""

for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):
  print("\nScore: {}\t \nTopic: {}".format(score, lda_model_tfidf.print_topic(index, 10)))

"""**coherence**"""

from gensim.models.coherencemodel import CoherenceModel

# Set up coherence model
coherence_model_lda = gensim.models.CoherenceModel(model=lda_model_tfidf, texts=processed_docs, dictionary=dictionary, coherence='u_mass')

# Calculate and print coherence
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score:', coherence_lda)

# Compute Coherence Score using UMass
coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=processed_docs, dictionary=dictionary, coherence="u_mass")
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Coherence values for varying alpha
def compute_coherence_values_ALPHA(corpus, dictionary, num_topics, seed, texts, start, limit, step):
  coherence_values = []
  model_list = []
  for alpha in range(start, limit, step):
    model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=seed, alpha=alpha/10, passes=100)
    model_list.append(model)
    coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')
    coherence_values.append(coherencemodel.get_coherence())
  return model_list, coherence_values

import matplotlib.pyplot as plt
model_list, coherence_values = compute_coherence_values_ALPHA(dictionary=dictionary, corpus=corpus_tfidf, num_topics=10, seed=10, texts=processed_docs, start=8, limit=20, step=2)
# Plot graph of coherence values by varying alpha
limit=20; start=8; step=2;
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()
"""
x_axis = []
for x in range(start, limit, step):
  x_axis.append(x/10)
plt.plot(x_axis, coherence_values)
plt.xlabel("Alpha")
plt.ylabel("Coherence score")
plt.legend(("coherence"), loc='best')
plt.show()
"""

"""def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    '''
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    '''
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values
"""



"""# **Testing model on unseen document**"""

unseen_document = 'How a Pentagon deal became an identity crisis for Google'
bow_vector = dictionary.doc2bow(preprocess(unseen_document))

for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):
  print("Score: {}\t Topic: {}".format(score, lda_model.print_topic(index, 5)))