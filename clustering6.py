# -*- coding: utf-8 -*-
"""clustering6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m8Q-8Ol0RgdhRJfVVoNYxtFDfn8QQ-qt

# 1.5
"""

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data

X = X[:50, :]
X.shape
(50, 4)

from sklearn.cluster import AgglomerativeClustering
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)

model = model.fit(X)

# Number of clusters
model.n_clusters_
50
# Distances between clusters
distances = model.distances_
distances.min()
0.09999999999999964
distances.max()
3.828052620290243

from scipy.cluster.hierarchy import dendrogram
from scipy.cluster import hierarchy

Z = hierarchy.linkage(model.children_, 'ward')

from matplotlib import pyplot as plt
plt.figure(figsize=(20,10))
dn = hierarchy.dendrogram(Z)

"""# **hierarchical**"""

pip install umap-learn

#pip install joblib==1.1.0

with open('w.txt') as f:
  lines = [line.rstrip('\n') for line in f]

embeddings=list()

import pandas as pd

embeddings=pd.read_csv('w.txt', sep='\t', header=0,encoding='UTF-8')

import umap
umap_embeddings = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine').fit_transform(embeddings)
#

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

feature_columns = ['1','2','3'	,'4'	,'5'	,'6',	'7',	'8',	'9',	'10',	'11',	'12',	'13',	'14',	'15',	'16',	'17',
                   '18',	'19',	'20',	'21',	'22',	'23',	'24',	'25',	'26',	'27',	'28',	'29',	'30',	'31',	'32',	'33',
                   '34',	'35',	'36',	'37',	'38',	'39',	'40',	'41',	'42',	'43',	'44',	'45',	'46',	'47',	'48',	'49',
                   '50',	'51',	'52',	'53',	'54',	'55',	'56',	'57',	'58',	'59',	'60',	'61',	'62',	'63',	'64',	'65',
                   '66',	'67',	'68',	'69',	'70',	'71',	'72',	'73',	'74',	'75',	'76',	'77',	'78',	'79',	'80',	'81',
                   '82',	'83',	'84',	'85',	'86',	'87',	'88',	'89',	'90',	'91',	'92',	'93', '94',	'95',	'96',	'97',
                   '98',	'99',	'100',	'101'	,'102',	'103',	'104',	'105',	'106',	'107',	'108',	'109',	'110',
                   '111',	'112',	'113',	'114',	'115',	'116',	'117',	'118',	'119',	'120']

X = embeddings[feature_columns]

import pandas as pd
import numpy as np

from sklearn.cluster import AgglomerativeClustering
model = AgglomerativeClustering(distance_threshold=0.82, n_clusters=None)

model = model.fit(X)

model.labels_  #####

# Number of clusters
model.n_clusters_
#50
# Distances between clusters
distances = model.distances_
distances.min()
#0.09999999999999964
distances.max()
#3.828052620290243

print(model.n_clusters_)  ####

print(distances)

print(distances.min())

print(distances.max())

from scipy.cluster.hierarchy import dendrogram
from scipy.cluster import hierarchy

Z = hierarchy.linkage(model.children_, 'ward')
print(Z)

from matplotlib import pyplot as plt
plt.figure(figsize=(10,10))
dn = hierarchy.dendrogram(Z)
#print(dn)

import matplotlib.pyplot as plt
import pandas as pd
# Prepare data
umap_data = umap.UMAP(n_neighbors=3, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) #کاهش بعد به 2
result = pd.DataFrame(umap_data, columns=['x', 'y'])
result['labels'] = model.labels_
print(result)

from sklearn.metrics import davies_bouldin_score
db_index = davies_bouldin_score(umap_embeddings, model.labels_)
print(db_index)

print(len(umap_embeddings))

print(len(model.labels_))

print(model.n_clusters_)

results = {}

for i in range(3,10):
   cluster = AgglomerativeClustering(distance_threshold=None, n_clusters=i)
   cluster = cluster.fit(X.to_numpy())

   umap_data = umap.UMAP(n_neighbors=2, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)
   result = pd.DataFrame(umap_data, columns=['x', 'y'])
  # print(cluster.labels_)
   result['labels'] = cluster.labels_
   db_index = davies_bouldin_score(umap_embeddings, cluster.labels_)
   print(db_index)
   results.update({i: db_index})

plt.plot(list(results.keys()), list(results.values()))
plt.xlabel("k")
plt.ylabel("Davies-Boulding Index")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
# %matplotlib inline

results = {}

for i in range(3,10):
    cluster = AgglomerativeClustering(distance_threshold=None, n_clusters=i)
    cluster = cluster.fit(X.to_numpy())
    umap_data = umap.UMAP(n_neighbors=2, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)
    result = pd.DataFrame(umap_data, columns=['x', 'y'])
    result['labels'] = cluster.labels_
    silhouette = silhouette_score(umap_embeddings, cluster.labels_)
    print(silhouette)
    results.update({i: silhouette})

plt.plot(list(results.keys()), list(results.values()))
plt.xlabel("k")
plt.ylabel("Silhouette")
plt.show()

from sklearn import metrics

results = {}

for i in range(3,10):
    cluster = AgglomerativeClustering(distance_threshold=None, n_clusters=i)
    cluster = cluster.fit(X.to_numpy())
    umap_data = umap.UMAP(n_neighbors=2, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)
    result = pd.DataFrame(umap_data, columns=['x', 'y'])
    result['labels'] =  cluster.labels_
    CH = metrics.calinski_harabasz_score(umap_embeddings, cluster.labels_)
    print(CH)
    results.update({i: CH})

plt.plot(list(results.keys()), list(results.values()))
plt.xlabel("k")
plt.ylabel("Calinski Harabasz")
plt.show()

"""# **BEFORE UMAP**"""

embedd=list()

import pandas as pd

embedd=pd.read_csv('w.txt', sep='\t', header=0,encoding='UTF-8')

#print(embedd[:121])

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

feature_columns = ['1','2','3'	,'4'	,'5'	,'6',	'7',	'8',	'9',	'10',	'11',	'12',	'13',	'14',	'15',	'16',	'17',
                   '18',	'19',	'20',	'21',	'22',	'23',	'24',	'25',	'26',	'27',	'28',	'29',	'30',	'31',	'32',	'33',
                   '34',	'35',	'36',	'37',	'38',	'39',	'40',	'41',	'42',	'43',	'44',	'45',	'46',	'47',	'48',	'49',
                   '50',	'51',	'52',	'53',	'54',	'55',	'56',	'57',	'58',	'59',	'60',	'61',	'62',	'63',	'64',	'65',
                   '66',	'67',	'68',	'69',	'70',	'71',	'72',	'73',	'74',	'75',	'76',	'77',	'78',	'79',	'80',	'81',
                   '82',	'83',	'84',	'85',	'86',	'87',	'88',	'89',	'90',	'91',	'92',	'93', '94',	'95',	'96',	'97',
                   '98',	'99',	'100',	'101'	,'102',	'103',	'104',	'105',	'106',	'107',	'108',	'109',	'110',
                   '111',	'112',	'113',	'114',	'115',	'116',	'117',	'118',	'119',	'120']

Z = embedd[feature_columns]

import pandas as pd
import numpy as np

from sklearn.cluster import AgglomerativeClustering
model = AgglomerativeClustering(distance_threshold=0.82, n_clusters=None)

model = model.fit(Z)

model.labels_  #####

# Number of clusters
model.n_clusters_
#50
# Distances between clusters
distances = model.distances_
distances.min()
#0.09999999999999964
distances.max()
#3.828052620290243

from scipy.cluster.hierarchy import dendrogram
from scipy.cluster import hierarchy

ZZ = hierarchy.linkage(model.children_, 'ward')
print(ZZ)

from matplotlib import pyplot as plt
plt.figure(figsize=(10,10))
dn = hierarchy.dendrogram(ZZ)
#print(dn)

import matplotlib.pyplot as plt
import pandas as pd
# Prepare data
##umap_data = umap.UMAP(n_neighbors=3, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) #کاهش بعد به 2
result = pd.DataFrame(embedd, columns=['x', 'y'])
result['labels'] = model.labels_

from sklearn.metrics import davies_bouldin_score
db_index = davies_bouldin_score(embedd, model.labels_)
print(db_index)

results = {}

for i in range(3,10):
   cluster = AgglomerativeClustering(distance_threshold=None, n_clusters=i)
   cluster = cluster.fit(X.to_numpy())

  # umap_data = umap.UMAP(n_neighbors=2, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)
   result = pd.DataFrame(embedd, columns=['x', 'y'])
  # print(cluster.labels_)
   result['labels'] = cluster.labels_
   db_index = davies_bouldin_score(embedd, cluster.labels_)
   print(db_index)
   results.update({i: db_index})

plt.plot(list(results.keys()), list(results.values()))
plt.xlabel("k")
plt.ylabel("Davies-Boulding Index")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn.metrics import silhouette_score
# %matplotlib inline

results = {}

for i in range(3,10):
   cluster = AgglomerativeClustering(distance_threshold=None, n_clusters=i)
   cluster = cluster.fit(X.to_numpy())

  # umap_data = umap.UMAP(n_neighbors=2, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)
   result = pd.DataFrame(embedd, columns=['x', 'y'])
  # print(cluster.labels_)
   result['labels'] = cluster.labels_
   silhouette = silhouette_score(embedd, cluster.labels_)
   print(silhouette)
   results.update({i: silhouette})

plt.plot(list(results.keys()), list(results.values()))
plt.xlabel("k")
plt.ylabel("Silhouette")
plt.show()

from sklearn import metrics

results = {}

for i in range(3,10):
   cluster = AgglomerativeClustering(distance_threshold=None, n_clusters=i)
   cluster = cluster.fit(X.to_numpy())

  # umap_data = umap.UMAP(n_neighbors=2, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)
   result = pd.DataFrame(embedd, columns=['x', 'y'])
  # print(cluster.labels_)
   result['labels'] = cluster.labels_
   CH = metrics.calinski_harabasz_score(embedd, cluster.labels_)
   print(CH)
   results.update({i: CH})

plt.plot(list(results.keys()), list(results.values()))
plt.xlabel("k")
plt.ylabel("Calinski Harabasz")
plt.show()