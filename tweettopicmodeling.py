# -*- coding: utf-8 -*-
"""TweetTopicModeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10TUf3cv4JnkT8ksiCcWLhJw1nPpFW2Th
"""

import pandas as pd

data=pd.read_csv('77_1.txt', sep='\t', header=0)

data_text = data[['Tweets']]   #***
data_text['index'] = data_text.index
documents = data_text

#Take a peek of the data.

print(len(documents))
print(documents[:6])

"""# **Data Pre-processing**"""

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *

import numpy as np
np.random.seed(2018)

import nltk
nltk.download('wordnet')
stemmer=SnowballStemmer

# function to perform lemmatize and stem preprocessing

stemmer = SnowballStemmer("english")

def lemmatize_stemming(text):
  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
  text = text.lower()
  text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
  text = re.sub(r'\@\w+|\#','', text)
  text = re.sub(r"covid|COVID|Covid|cOVID",'', text)
  text = re.sub(r"corona|Corona|CORONA|cORONA",'', text)
  text = re.sub(r"coronavirus|Coronavirus|CORONAVIRUS|cORONAVIRUS",'', text)
  text = re.sub(r"VIRUS|Virus|vIRUS|virus",'', text)
  text = re.sub(r"trump|Trump",'', text)
  result = []
  for token in gensim.utils.simple_preprocess(text):
    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 :   ###***
      result.append(lemmatize_stemming(token))
  return result

#Select a document to preview after preprocessing

doc_sample = documents[documents['index'] == 1].values[0][0]

print('original document: ')
words = []
for word in doc_sample.split(' '):
  words.append(word)
print(words)
print('\n\n tokenized and lemmatized document: ')
print( preprocess(doc_sample) )

"""# **Bag of Words on the Data set**

Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set.
"""

#Preprocess the headline text, saving the results as ‘processed_docs’

processed_docs = documents['Tweets'].map(preprocess)
processed_docs[150]

dictionary = gensim.corpora.Dictionary(processed_docs)

count = 0
for k, v in dictionary.iteritems():
    print(k, v)
    count += 1
    if count > 15:
        break

"""***Gensim filter_extremes***

**Filter out tokens that appear in**

less than 15 documents (absolute number) or
more than 0.5 documents (fraction of total corpus size, not absolute number).
after the above two steps, keep only the first 100000 most frequent tokens.
"""

dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)

#Gensim doc2bow

bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
bow_corpus[:3]

#Preview Bag Of Words for our sample preprocessed document

bow_doc_4310 = bow_corpus[8]
for i in range(len(bow_doc_4310)):
  print("Word {} (\"{}\") appears {} time.".format(bow_doc_4310[i][0],dictionary[bow_doc_4310[i][0]], bow_doc_4310[i][1]))

"""# **TF-IDF**

Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document.
"""

from gensim import corpora, models   #****

tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]

from pprint import pprint

for doc in corpus_tfidf:
  pprint(doc)
  break

"""# **Running LDA using Bag of Words**

*   List item
*   List item



Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’
"""

lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)

#For each topic, we will explore the words occuring in that topic and its relative weight

for idx, topic in lda_model.print_topics(-1):
  print('Topic: {} \nWords: {}'.format(idx, topic))

""" # **Running LDA using TF-IDF**

 تاپیک های برتر
"""

lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)

tt=open('Topics.txt','w')

for idx, topic in lda_model_tfidf.print_topics(-1):
  print('Topic: {} Word: {}'.format(idx, topic))
  tt.write('{} {}'.format(idx, topic) + '\n')

"""# **Performance evaluation by classifying sample document using LDA Bag of Words model**


"""

processed_docs[5]

for index, score in sorted(lda_model[bow_corpus[5]], key=lambda tup: -1*tup[1]):
  print("\nScore: {}\t \nTopic: {}".format(score, lda_model.print_topic(index, 20)))

"""# **Performance evaluation by classifying sample document using LDA TF-IDF model.**


"""

for index, score in sorted(lda_model_tfidf[bow_corpus[1]], key=lambda tup: -1*tup[1]):
  print("\nScore: {}\t \nTopic: {}".format(score, lda_model_tfidf.print_topic(index, 10)))

"""# **Testing model on unseen document**"""

unseen_document = 'my grandpa died because of coronavirus'
bow_vector = dictionary.doc2bow(preprocess(unseen_document))

for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):
  print("Score: {}\t Topic: {}".format(score, lda_model.print_topic(index, 10))) #10 words of Topic