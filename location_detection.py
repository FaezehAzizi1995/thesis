# -*- coding: utf-8 -*-
"""Copy of location_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f0sZ7hz1bytXohjXMZsyPlj5gOB9UwzH
"""

#********************   Connect Dataset   **************************
!pip install PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':""})
downloaded.GetContentFile('covid_tweets.tsv')

downloaded

import pandas as pd
data = pd.read_table('covid_tweets.tsv')

data.head()
#**********************   ToKenize Dataset   **************************

tweet_file=open('covid_tweets.tsv','r')

all_tweets=list()
all_tokenize = list()

Val = 0

for line in tweet_file:
  all_tweets.append(line.split('\n')[0].split('\t')[2])

from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()

for item in all_tweets:
  tokens = tknzr.tokenize(all_tweets[Val])
  all_tokenize.append(tokens)
 # print(all_tokenize[Val])
  Val=Val+1

#********************   Tweets Contain City   *********************

all_city=dict()

all_city_file=open('city.txt','r')

for line in all_city_file:
  all_city[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.lower() in all_city:
    #  print(all_tweets[i])
      break
#*******************   Tweets Contain Country   **********************

all_country=dict()

all_country_file=open('country.txt','r')

for line in all_country_file:
  all_country[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.upper() in all_country:
      print(all_tweets[i])
      break

#****************     Tweets Contain Country Adjective   ****************

all_country_adj=dict()

all_country_adj_file=open('country_adj.txt','r')

for line in all_country_adj_file:
  all_country_adj[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.lower() in all_country_adj:
#      print(all_tweets[i])
      break
#*********************   Tweets Contain Province   *********************

all_province=dict()

all_province_file=open('province.txt','r')

for line in all_province_file:
  all_province[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.lower() in all_province:
#      print(all_tweets[i])
      break


city=dict()
country=dict()
country_adj=dict()
province=dict()

tweet_file=open('tweet-fear.txt','r')

all_tweets=list()
all_tokenize = list()


for line in tweet_file:
 all_tweets.append(line.split('\n')[0].split('\t')[2])

from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()

for tweet in all_tweets:
 tokens = tknzr.tokenize(tweet)
 all_tokenize.append(tokens)
 #print(all_tokenize)


#******************* countrys ****************
#print(all_tokenize)
countrys=dict()

countrys_file=open('countrys.txt','r')


for line in countrys_file:
 countrys[line.split('\n')[0]]=1

country= list()

for tweet in all_tokenize:
 for token in tweet:
  if token in countrys:
   country.append(token.split('\n')[0])


#print(countrys)

i=0
for c in countrys:
 print(c)
 for d in country:
  if c==d:
   i = i + 1
 print(d)
 print(i)
 print("\n")
 i=0



#تمرین کردن در این قسمت
!pip install PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':""})
downloaded.GetContentFile('covid_tweets.tsv')
downloaded
import pandas as pd
data=pd.read_table('covid_tweets.tsv')
data.head()

downloaded_city = drive.CreateFile({'id':""})
downloaded_city.GetContentFile('city.txt')
downloaded_city
import pandas as pd_city
data_city=pd_city.read_table('covid_tweets.tsv')
data_city.head()



#####
city_file=open('city.txt','r')

tweet_file=open('covid_tweets.tsv','r')

all_tweets=list()
all_tokenize = list()
all_city=list()
#all_words = list()
y=0
Val = 0
vall=0
for line in tweet_file:
  all_tweets.append(line.split('\n')[0].split('\t')[2])


for line in city_file:
  all_city.append(city_file.readline())

from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()

val=0
for item in all_tweets:
  tokens = tknzr.tokenize(all_tweets[Val])
  all_tokenize.append(tokens)
  Val=Val+1

temp = ""
for i in range(len(all_tokenize)):
  for k in range(len(all_city)):
    temp = all_city[k]
    if all_tokenize[i].count(temp) > 0 :
      print(all_tokenize[i])

city=dict()
country=dict()
country_adj=dict()
province=dict()

#**********************   ConnectDataset   **************************
!pip install PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':""})
downloaded.GetContentFile('covid_tweets.tsv')

downloaded

import pandas as pd
data = pd.read_table('covid_tweets.tsv')

data.head()
#**********************   ToKenizeDataset   **************************

tweet_file=open('covid_tweets.tsv','r')

all_tweets=list()
all_tokenize = list()
Val = 0

for line in tweet_file:
  all_tweets.append(line.split('\n')[0].split('\t')[2])

from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()

for item in all_tweets:
  tokens = tknzr.tokenize(all_tweets[Val])
  all_tokenize.append(tokens)
  #print(all_tokenize[Val])
  Val=Val+1

###############################################################################


all_city=dict()

all_city_file=open('city.txt','r')

for line in all_city_file:
  all_city[line.split('\n')[0]]=1



i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.lower() in all_city:
      print(all_tweets[i])
      break


city=dict()
country=dict()
country_adj=dict()
province=dict()

#********************   Connect Dataset   **************************
!pip install PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':"1qUa8InhENlV0vZXpPFABoNwysIs14hKN"})
downloaded.GetContentFile('covid_tweets.tsv')

downloaded

import pandas as pd
data = pd.read_table('covid_tweets.tsv')

data.head()
#**********************   ToKenize Dataset   **************************

tweet_file=open('covid_tweets.tsv','r')

all_tweets=list()
all_tokenize = list()

Val = 0

for line in tweet_file:
  all_tweets.append(line.split('\n')[0].split('\t')[2])

from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()

for item in all_tweets:
  tokens = tknzr.tokenize(all_tweets[Val])
  all_tokenize.append(tokens)
 # print(all_tokenize[Val])
  Val=Val+1

#********************   Tweets Contain City   *********************

all_city=dict()

all_city_file=open('fear2.txt','r')

for line in all_city_file:
  all_city[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.lower() in all_city:
    #  print(all_tweets[i])
      break

#**********************   ToKenize Dataset   **************************

tweet_file=open('out.txt','r')

all_tweets=list()
all_tokenize = list()

Val = 0

for line in tweet_file:
  all_tweets.append(line.split('\n')[0].split('\t')[2])

from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()

for item in all_tweets:
  tokens = tknzr.tokenize(all_tweets[Val])
  all_tokenize.append(tokens)
 # print(all_tokenize[Val])
  Val=Val+1

#********************   Tweets Contain City   *********************

all_city=dict()

all_city_file=open('city.txt','r')

for line in all_city_file:
  all_city[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.lower() in all_city:
    #  print(all_tweets[i])
      break
#*******************   Tweets Contain Country   **********************

all_country=dict()

all_country_file=open('country.txt','r')

for line in all_country_file:
  all_country[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.upper() in all_country:
      print(all_tweets[i])
      break

#****************     Tweets Contain Country Adjective   ****************

all_country_adj=dict()

all_country_adj_file=open('country_adj.txt','r')

for line in all_country_adj_file:
  all_country_adj[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.lower() in all_country_adj:
#      print(all_tweets[i])
      break
#*********************   Tweets Contain Province   *********************

all_province=dict()

all_province_file=open('province.txt','r')

for line in all_province_file:
  all_province[line.split('\n')[0]]=1


i=-1
for tokenized_tweet in all_tokenize:
  i=i+1
  for token in tokenized_tweet:
    if token.lower() in all_province:
#      print(all_tweets[i])
      break


city=dict()
country=dict()
country_adj=dict()
province=dict()

tweet_file=open('out.txt','r')

all_tweets=list()
all_tokenize = list()


for line in tweet_file:
all_tweets.append(line.split('\n')[0])

from nltk.tokenize import TweetTokenizer
tknzr = TweetTokenizer()

for tweet in all_tweets:
tokens = tknzr.tokenize(tweet)
all_tokenize.append(tokens)
#print(all_tokenize)


#******************* countrys ****************
#print(all_tokenize)
countrys=dict()

countrys_file=open('country.txt','r')


for line in countrys_file:
countrys[line.split('\n')[0]]=1

country= list()

for tweet in all_tokenize:
for token in tweet:
if token in countrys:
country.append(token.split('\n')[0])


#print(countrys)

i=0
for c in countrys:
print(c)
for d in country:
if c==d:
i = i + 1
print(i)
print("\n")
i=0